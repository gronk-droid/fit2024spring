{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE4224 Project 2\n",
    "#### Grant Butler | gbutler2020@my.fit.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Retrieval/Cleaning:\n",
    "Here, I will use the Spotify API to get the audio features of the tracks in my library, and create a cleaned pandas dataframe to be used with PCA and t-SNE after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# use dotenv to import api creds\n",
    "secrets = dotenv_values(\".env\")\n",
    "\n",
    "# making authentication manager for spotipy to communicate with spotify\n",
    "auth_manager = SpotifyClientCredentials(client_id=secrets[\"SPOTIPY_CLIENT_ID\"],\n",
    "                                        client_secret=secrets[\"SPOTIPY_CLIENT_SECRET\"])\n",
    "\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)  # making spotipy object\n",
    "sp.trace = False  # no debugging needed\n",
    "\n",
    "# grabbing the track ids and adding them to an array passed into them\n",
    "def show_tracks(results, uriArray):\n",
    "    for i, item in enumerate(results['items']):\n",
    "        track = item['track']\n",
    "        uriArray.append(track['id'])\n",
    "\n",
    "# taking in the tracks from the playlist and grabbing the ids before returning them to a list\n",
    "def get_playlist_track_ids(username, playlist_id):\n",
    "    track_ids = []\n",
    "    playlist = sp.user_playlist(username, playlist_id)\n",
    "\n",
    "    tracks = playlist['tracks']\n",
    "\n",
    "    while tracks['next']:\n",
    "        tracks = sp.next(tracks)\n",
    "        show_tracks(tracks, track_ids)\n",
    "    return track_ids\n",
    "\n",
    "\n",
    "track_ids = get_playlist_track_ids(secrets['SPOTIFY_USERNAME'],\n",
    "                                   secrets['PLAYLIST_ID'])\n",
    "\n",
    "print(track_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# helper function to split up the bigger list of track ids (spotify api limits to 100 tracks)\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# taking in a chunk at a time, ensuring they are strings, and then adding on the features and returning them\n",
    "def get_audio_features(track_ids):\n",
    "    audio_features = []\n",
    "    for chunk in chunks(track_ids, 100):\n",
    "        chunk = [str(track_id) for track_id in chunk]\n",
    "        audio_features.extend(sp.audio_features(chunk))\n",
    "    return audio_features\n",
    "\n",
    "audio_features = get_audio_features(track_ids)\n",
    "print(json.dumps(audio_features, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ensuring there are no entries without data\n",
    "cleaned_features = [item for item in audio_features if item is not None]\n",
    "\n",
    "# making dataframe from the list of dicts\n",
    "df = pd.DataFrame(cleaned_features)\n",
    "\n",
    "# remove fields that have no bearing on analysis\n",
    "fields_to_remove = [\"analysis_url\", \"track_href\", \"type\", \"uri\"]\n",
    "df = df.drop(columns=fields_to_remove)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Dimensional Reduction:\n",
    "\n",
    "Using PCA, I hope to reduce the number of dimensions that t-SNE needs to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df_numeric = df.drop(columns='id')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_numeric)\n",
    "\n",
    "# apply PCA\n",
    "pca = PCA(n_components=2) # only 2 dimensions for visualization\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# visualize PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.8)\n",
    "plt.title('PCA Visualization')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking loadings of principal components\n",
    "loadings = pca.components_\n",
    "\n",
    "loadings_df = pd.DataFrame(loadings, columns=df_numeric.columns)\n",
    "\n",
    "print(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sort the loadings to be the most impactful first\n",
    "\n",
    "loadings_diff = np.abs(np.diff(loadings, axis=0))\n",
    "loadings_diff_sum = loadings_diff.sum(axis=0)\n",
    "loadings_diff_df = pd.DataFrame(\n",
    "    loadings_diff_sum, index=df_numeric.columns, columns=['Sum of Loadings Diff'])\n",
    "\n",
    "loadings_diff_sorted = loadings_diff_df.sort_values(by='Sum of Loadings Diff', ascending=False)\n",
    "\n",
    "print(loadings_diff_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Print the explained variance ratio for each component\n",
    "for i, ratio in enumerate(explained_variance_ratio):\n",
    "    print(f\"Principal Component {i + 1}: Explained Variance Ratio = {ratio:.4f}\")\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(explained_variance_ratio), marker='o', linestyle='-')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# using 2 components for t-SNE for visualization after reducing with PCA\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "# t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.8)\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using k-means to group the clusters and color them based on that\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# apply k-means to t-SNE results\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_tsne)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster_id in range(len(np.unique(clusters))):\n",
    "    cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "    plt.scatter(X_tsne[cluster_indices, 0], X_tsne[cluster_indices,\n",
    "                1], label=f'Cluster {cluster_id}', alpha=0.8)\n",
    "plt.title('t-SNE Visualization (Clustered)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "# plt.legend() # legends for whether you see which cluster is which\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cluster_track_ids = {}\n",
    "for cluster_label, track_id in zip(clusters, track_ids):\n",
    "    if cluster_label not in cluster_track_ids:\n",
    "        cluster_track_ids[cluster_label] = []\n",
    "    cluster_track_ids[cluster_label].append(track_id)\n",
    "\n",
    "# relabel points with original components\n",
    "for cluster_label, track_ids in cluster_track_ids.items():\n",
    "    print(f'Cluster {cluster_label}:')\n",
    "    for track_id in track_ids:\n",
    "        index = track_ids.index(track_id)\n",
    "        original_point = X_tsne[index]\n",
    "        print(f'\\tTrack ID: {track_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse4224_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
